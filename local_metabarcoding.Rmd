---
title: "iMapPESTS local metabarcoding workflow"
author: "A.M. Piper"
date: "2020/02/10"
output:
  html_document:
    highlighter: null
    theme: "flatly"
    code_download: true
    toc: true
    toc_float: 
      collapsed: false
      smooth_scroll: true
    df_print: paged    
  pdf_document: default
  
---

```{r setup, include=FALSE}
# Knitr global setup - change eval to true to run code
library(knitr)
knitr::opts_chunk$set(echo = TRUE, eval=FALSE, message=FALSE,error=FALSE,fig.show = "hold", fig.keep = "all")
opts_chunk$set(dev = 'png')
```

# Introduction 

```{r install & Load packages} 
#Set required packages
.cran_packages <- c("ggplot2",
                    "gridExtra",
                    "tidyverse", 
                    "scales",
                    "stringdist",
                    "patchwork",
                    "vegan",
                    "ggpubr",
                    "seqinr",
                    "patchwork")
.bioc_packages <- c("dada2",
                    "phyloseq",
                    "DECIPHER",
                    "Biostrings",
                    "ShortRead")

.inst <- .cran_packages %in% installed.packages()
if(any(!.inst)) {
   install.packages(.cran_packages[!.inst])
}
.inst <- .bioc_packages %in% installed.packages()
if(any(!.inst)) {
  if (!requireNamespace("BiocManager", quietly = TRUE))
    install.packages("BiocManager")
  BiocManager::install(.bioc_packages[!.inst], ask = F)
}

#Load all packages
sapply(c(.cran_packages,.bioc_packages), require, character.only = TRUE)

devtools::install_github("alexpiper/taxreturn")

library(taxreturn)

#Install bbmap
taxreturn::bbmap_install(dest.dir = "bin")

#Install fastqc
taxreturn::fastqc_install(dest.dir = "bin")

#Install blast
taxreturn::blast_install(dest.dir = "bin")
```

## Create Sample Sheet

We will now use the Illumina run sample sheet, and runparameters files to make a new samplesheet that we will use to track samples through the pipeline. We will add the primer sequences as well for our later demultiplexing steps:

FORWARD PRIMERS:
    Name                    Illumina overhang adapter           Primer sequences
    fwhF2_P5	ACACTCTTTCCCTACACGACGCTCTTCCGATCT	  GGDACWGGWTGAACWGTWTAYCCHCC
    
REVERSE PRIMER:
    Name                    Illumina overhang adapter           Primer sequences
    fwhR2nT1_P7	GTGACTGGAGTTCAGACGTGTGCTCTTCCGATCT	CGTRATWGCHCCDGCTARWACWGG
    

If we use a heirarchial tagging strategy we can also add these here

```{r Create Samplesheet}

samdf <- taxreturn::create_samplesheet(SampleSheet = "sample_data/SampleSheet.csv",
                          runParameters = "sample_data/runParameters.xml",
                          format="miseq")

samdf <- samdf %>%
  mutate(Fprimer = "GGDACWGGWTGAACWGTWTAYCCHCC",
         Rprimer = "CGTRATWGCHCCDGCTARWACWGG", 
         twintagged = FALSE)


write_csv(samdf, "sample_data/Sample_info.csv")
```


#Quality checks:

We will conduct 2 quality checks. Firstly a check of the entire sequence run level quality check to identify any anomalies, followed by a sample level quality check to identify potential issues with specific samples.

```{r quality checks}
#Sequencing run quality using BasecallQC package * NOT CURRENTLY IMPLEMENTED *

## Sample quality using fastqc
library(ngsReports)
test <- taxreturn::fastqc(fq.dir = trimmedpath, threads=2)

# ngsreports of fastqc
fileDir <- file.path("data/run_4/FASTQC")
writeHtmlReport(fileDir, overwrite = TRUE, quiet=FALSE)
```


# Demultiplex by primer & Trim

DADA2 requires Non-biological nucleotides i.e. primers, adapters, linkers, etc to be removed. Following demultiplhowever primer sequences still remain in the reads and must be removed prior to use with the DADA2 algorithm.

For this workflow we will be using the Kmer based adapter trimming software BBDuk (Part of BBTools package https://jgi.doe.gov/data-and-tools/bbtools/) to trim the primers from our raw data files.

If a heirarchial multiplexing strategy was used where multiple sets of samples or loci were labelled with the same illumina indexes, for example when targetting both COI and a pathogen loci, we can use the primer sequences that were used to amplify each loci to further demultiplex the data. For this to occur, make sure that twintagged is set to TRUE in the samplesheet

## fwhF2-fwhR2n amplicon:

    
```{r primer trimming , message=FALSE}
#Load sample sheet
samdf <- read.csv("sample_data/Sample_info.csv", stringsAsFactors = FALSE)

#Demultiplex samples
runs <- dir("data/", pattern="run")

i=1

#Create vectors to track reads
trimmed <- vector("list", length=length(runs))
demux <- vector("list", length=length(runs))

for (i in seq(along=runs)){
  path <- paste0("data/", runs[i]) # CHANGE ME to the directory containing your demultiplexed forward-read fastq files
  
  run_data <- samdf %>%
    filter(FCID== runs[i] %>% str_split_fixed(pattern="_", n=2) %>% as_tibble() %>% pull(V2))
  
  #Get primer sequences
  primers <- c(unique(run_data$Fprimer), unique(run_data$Rprimer))
  
  #Check if samples were twin tagged - these require extra round of demultiplexing
  twintagged <- any(!is.na(run_data$twintagF))
  if (twintagged == TRUE) {
      demuxpath <- file.path(path, "demux") # Filtered forward files go into the path/filtered/ subdirectory
      dir.create(demuxpath)
      
      fastqFs <- sort(list.files(path, pattern="R1_001.*", full.names = TRUE))
      fastqRs <- sort(list.files(path, pattern="R2_001.*", full.names = TRUE))
      
      demux[[i]] <- bbdemux(install="bin/bbmap", fwd=fastqFs, rev=fastqRs, Fbarcodes = unique(run_data$twintagF),
                    Rbarcodes = unique(run_data$twintagR), degenerate=TRUE, out.dir=demuxpath, threads=1 ,
                    mem=4,  hdist=0, overwrite=TRUE, tidylog = TRUE)
      
      demux_fastqs <- sort(list.files(paste0(demuxpath), pattern="_R1R2_", full.names = TRUE))
    
      trimmed[[i]] <- bbtrim(install="bin/bbmap", fwd=demux_fastqs, 
                    primers = primers, 
                    degenerate = TRUE, out.dir="trimmed", trim.end = "left",
                    ordered=TRUE, mink=FALSE, hdist=2, 
                    maxlength= (max(run_data$readlength) - sort(nchar(primers), decreasing=FALSE)[1]) +5,
                    overwrite=TRUE, quality=FALSE, tidylog=TRUE)
      
      #Re-split interleaved fastq's
      trimmedpath <- file.path(demuxpath, "trimmed") # Filtered forward files go into the path/filtered/ subdirectory
      trimmed_fastqs <- sort(list.files(trimmedpath, pattern="_R1R2_", full.names = TRUE))
      bbsplit(install="bin/bbmap", files=trimmed_fastqs, overwrite=TRUE)
  
      
  } else if (twintagged == FALSE) {
    
    fastqFs <- sort(list.files(paste0(path), pattern="_R1_", full.names = TRUE))
    fastqRs <- sort(list.files(paste0(path), pattern="_R2_", full.names = TRUE))
    
    trimmed[[i]] <- bbtrim(install="bin/bbmap", fwd=fastqFs, rev=fastqRs,
                    primers=primers, 
                    degenerate = TRUE, out.dir="trimmed", trim.end = "left",
                    ordered=TRUE, mink=FALSE, hdist=2,
                    maxlength=(max(run_data$readlength) - sort(nchar(primers), decreasing=FALSE)[1]) +5,
                    overwrite=TRUE, quality=FALSE, tidylog=TRUE)
    }
}
  
write_tsv(bind_rows(demux), "logs/demux.tsv")
write_tsv(bind_rows(trimmed), "logs/trimmed.tsv")
  
```
  
  
## Plot read quality & lengths

```{r QA plot, eval = FALSE, cache= TRUE}
runs <- dir("data/", pattern="run")
#Load sample sheet
samdf <- read.csv("sample_data/Sample_info.csv", stringsAsFactors = FALSE)


for (i in seq(along=runs)){
  run_data <- samdf %>%
    filter(FCID== runs[i] %>% str_split_fixed(pattern="_", n=2) %>% as_tibble() %>% pull(V2))
  
  #Check if run used twin tags
  twintagged <- any(!is.na(run_data$twintagF))
  
  if (twintagged == TRUE) {
    path <- paste0("data/", runs[i], "/demux/trimmed" )# CHANGE ME to the directory containing primer trimmed fastq files
  } else if (twintagged == FALSE) {
    path <- paste0("data/", runs[i], "/trimmed" )# CHANGE ME to the directory containing primer trimmed fastq files
  }
 
  ##Get trimmed files, accounting for empty files (28 indicates empty sample)
  trimmedFs <- sort(list.files(path, pattern="_R1_", full.names = TRUE))
  trimmedFs <- trimmedFs[file.size(trimmedFs)>28]

  #Plot an aggregate quality of random samples
  sampleF <- sample(trimmedFs, 12)
  sampleR <- sampleF %>% str_replace(pattern="_R1_", replacement = "_R2_")
  
  p1 <- plotQualityProfile(sampleF, aggregate = FALSE) + ggtitle(paste0(runs[i], " Forward Reads")) 
  p2 <- plotQualityProfile(sampleR, aggregate = FALSE) + ggtitle(paste0(runs[i], " Reverse Reads"))
  
  #output plots
  dir.create("output")
  dir.create("output/figures/")
  pdf(paste0("output/figures/", runs[i], "_prefilt_quality.pdf"), width = 11, height = 8 , paper="a4r")
  plot(p1)
  plot(p2)
  dev.off()
}

```

In gray-scale is a heat map of the frequency of each quality score at each base position. The median quality score at each position is shown by the green line, and the quartiles of the quality score distribution by the orange lines. The red line shows the scaled proportion of reads that extend to at least that position (this is more useful for other sequencing technologies, as Illumina reads are typically all the same lenghth, hence the flat red line).

The forward reads are good quality. We generally advise trimming the last few nucleotides to avoid less well-controlled errors that can arise there. These quality profiles do not suggest that any additional trimming is needed. We will truncate the forward reads at position 240 (trimming the last 10 nucleotides).

The reverse reads are of significantly worse quality, especially at the end, which is common in Illumina sequencing. This isn’t too worrisome, as DADA2 incorporates quality information into its error model which makes the algorithm robust to lower quality sequence, but trimming as the average qualities crash will improve the algorithm’s sensitivity to rare sequence variants. Based on these profiles, we will truncate the reverse reads at position 160 where the quality distribution crashes.

## Filter and trim

The max expected error function is used as the primary quality filter, and all reads containing N bases were removed. Should be using trunclength to make sure the amplicons are the same length despite heterogeneity filtering!


```{r filter and trim}
runs <- dir("data/", pattern="run")
filtered_out <- vector("list", length=length(runs))

samdf <- read.csv("sample_data/Sample_info.csv", stringsAsFactors = FALSE)

for (i in 1:length(runs)){
  
   run_data <- samdf %>%
    filter(FCID== runs[i] %>% str_split_fixed(pattern="_", n=2) %>% as_tibble() %>% pull(V2))
  
  #Check if run used twin tags
  twintagged <- any(!is.na(run_data$twintagF))
  
  if (twintagged == TRUE) {
    path <- paste0("data/", runs[i], "/demux/trimmed" )
  } else if (twintagged == FALSE) {
    path <- paste0("data/", runs[i], "/trimmed" )
  }

  filtpath <- file.path(path, "filtered") # Filtered forward files go into the path/filtered/ subdirectory
  dir.create(filtpath)
  fastqFs <- sort(list.files(path, pattern="R1_001.*"))
  fastqRs <- sort(list.files(path, pattern="R2_001.*"))
  
  if(length(fastqFs) != length(fastqRs)) stop(paste0("Forward and reverse files for ",runs[i]," do not match."))
  
  filtered_out[[i]] <- filterAndTrim(fwd=file.path(path, fastqFs), filt=file.path(filtpath, fastqFs),
                                      rev=file.path(path, fastqRs), filt.rev=file.path(filtpath, fastqRs),
                                      maxEE=2, truncLen = 120, maxN = 0,
                                      rm.phix=TRUE, rm.lowcomplex=0,
                                      multithread=TRUE, compress=TRUE, verbose=TRUE)

  # post filtering plot
  filtFs <- sort(list.files(filtpath, pattern="R1_001.*", full.names = TRUE))
  #filtRs <- sort(list.files(filtpath, pattern="R2_001.*", full.names = TRUE))
  
  sampleF <- sample(filtFs, 12)
  sampleR <- sampleF %>% str_replace(pattern="R1_001", replacement = "R2_001")
  
  p1 <- plotQualityProfile(sampleF, aggregate = FALSE) + ggtitle(paste0(runs[i]," Forward Reads")) 
  p2 <- plotQualityProfile(sampleR, aggregate = FALSE) + ggtitle(paste0(runs[i]," Reverse Reads"))
  
  #output plots
  dir.create("output/figures/")
  pdf(paste0("output/figures/",runs[i],"_postfilt_quality.pdf"), width = 11, height = 8 , paper="a4r")
  plot(p1)
  plot(p2)
  dev.off()
}

filtered_out %>%
  map(as_tibble, rownames=NA) %>%
  map(rownames_to_column, var="Sample") %>%
  bind_rows() %>%
  write_tsv("logs/filtered.tsv")
  print(filtered_out)

```


# Sequence processing

## Infer sequence variants for each run

This workflow uses the DADA2 algorithm to differentiate real sequences from error using their abundance and co-occurance patters. This relies on the assumption that assuming a random error process where base errors are introduced randomly by either PCR polymerase or sequencing, real sequences will be high quality in the same way, while bad sequences are bad in different individual ways.

DADA2 depends on a parameterized error model (the 16(possible bases) × 41(phred score) transition probabilities, for example, p(A→C, 35)), which is estimated from the data. DADA2’s default parameter estimation method is to perform a weighted loess fit to the regularized log of the observed mismatch rates as a function of their quality, separately for each transition type (for example, A→C mismatches are fit separately from A→G mismatches).

Every amplicon dataset has a different set of error rates and the DADA2 algorithm makes use of a parametric error model (err) to model this and infer real biological sequence variation from error. Following error model learning, all identical sequencing reads are dereplicated into into “Exact sequence variants” with a corresponding abundance equal to the number of reads with that unique sequence. The forward and reverse reads are then merged together by aligning the denoised forward reads with the reverse-complement of the corresponding reverse reads, and then constructing the merged “contig” sequences. Following this step, a sequence variant table is constructed and saved as an RDS file.

The problem with novaseq data is the binned quality scores.

NovaSeq error rate conversions

0-2 -> 2
3-14 -> 12
15-30 -> 23
31-40 -> 37

For this analysis we will use all the reads to estimate error rate, and plot the error model for each run as a sanity check. In this plot you generally want to see if the fitted error rates (black line) reasonably fit the observations (black points) and generally decrease with increasing Q (towards right of plot)?

For high depth sequencing such as with the Novaseq, i would suggest increasing the value of nbases to ensure the error rate is inferred from a suitable proportion of the data
```{r Learn error rates }
runs <- dir("data/", pattern="run")
set.seed(100)

samdf <- read.csv("sample_data/Sample_info.csv", stringsAsFactors = FALSE)

for (i in seq(along=runs)){
  
  run_data <- samdf %>%
    filter(FCID == runs[i] %>% str_split_fixed(pattern="_", n=2) %>% as_tibble() %>% pull(V2))
  
  #Check if run used twin tags
  twintagged <- any(!is.na(run_data$twintagF))
  if (twintagged == TRUE) {
    filtpath <- paste0("data/", runs[i], "/demux/trimmed/filtered" )
  } else if (twintagged == FALSE) {
    filtpath <- paste0("data/", runs[i], "/trimmed/filtered" )
  }
  
  filtFs <- list.files(filtpath, pattern="R1_001.*", full.names = TRUE)
  filtRs <- list.files(filtpath, pattern="R2_001.*", full.names = TRUE)
  
  # Learn error rates from a subset of the samples and reads (rather than running self-consist with full dataset)
  # nread tells the function how many reads to use in error learning, this can be increased for more accuracy at the expense of runtime
  
  errF <- learnErrors(filtFs, multithread = 20, nbases = 1e+09, randomize = TRUE, qualityType = "FastqQuality", verbose=TRUE)
  errR <- learnErrors(filtRs, multithread = 20, nbases = 1e+09, randomize = TRUE, qualityType = "FastqQuality", verbose=TRUE)
  
  ##Print error plots to see how well the algorithm modelled the errors in the different runs
  print(plotErrors(errF, nominalQ=TRUE) + ggtitle(paste0(runs[i], " Forward Reads")))
  print(plotErrors(errR, nominalQ=TRUE) + ggtitle(paste0(runs[i], " Reverse Reads")))
  
  #check if any run uses a platform with binned quality scores - if so enforce monotonicity
  binnedqual <-  any(run_data$seq_platform %in% c("Novaseq", "Nextseq"))
  if (binnedqual == TRUE){
  enforce_mono <- function(err){
    err.mat <- getErrors(err, detailed=TRUE)
    for(trans in c("A2C", "A2G", "A2T", "C2A", "C2G", "C2T", "G2A", "G2C", "G2T", "T2A", "T2C", "T2G")) {
      #Transform each error rate that is below the model value at the max Q score (40) to the model value at that max Q score.
      err.mat$err_out[trans,] <- pmax(err.mat$err_out[trans,], err.mat$err_out[trans,ncol(err.mat$err_out)])
    }
    return(err.mat)
  }
  
  mono.errmatF <- enforce_mono(errF)
  mono.errmatR <- enforce_mono(errR)
  
  print(plotErrors(mono.errmatF, nominalQ=TRUE)+ ggtitle("Monotonicity enforced Forward Reads"))
  print(plotErrors(mono.errmatR, nominalQ=TRUE)+ ggtitle("Monotonicity enforced Reverse Reads"))

  errF <- mono.errmatF
  errR <- mono.errmatR
  }
  
  #Error inference and merger of reads - Using pseudo pooling for increased sensitivity
  dadaFs <- dada(filtFs, err=errF, multithread=20, pool="pseudo", verbose=TRUE)
  dadaRs <- dada(filtRs, err=errR, multithread=20, pool="pseudo", verbose=TRUE)
 
  mergers <- mergePairs(dadaFs, filtFs, dadaRs, filtRs, verbose=TRUE, minOverlap = 12)
  
  # Construct sequence table
  dir.create("output/rds/")
  seqtab <- makeSequenceTable(mergers)
  saveRDS(seqtab, paste0("output/rds/",runs[i], "_seqtab.rds"))
}
```

## Merge Runs, Remove Chimeras and filter

All the below filters increase the proportion of reads classified to lower levels compared to higher levels

```{r merge runs and remove chimeras}
seqtabs <- list.files("output/rds/", pattern="seqtab.rds", full.names = TRUE)

#Read in and rename undetermined - using dummy variables
for (i in seq(along=seqtabs)){
  assign(paste("st", i, sep = ""), readRDS(seqtabs[i]))
  labelling <- get(paste("st", i, sep = ""))
  labels <- rownames(labelling)%>%
    str_replace(pattern="Undetermined_", replacement = paste("Undetermined_",i,"_"))
  rownames(labelling)<-labels
  assign(paste("st", i, sep = ""), labelling)
}

st.all <- mergeSequenceTables(st1, st2, st3, st4, st5)

#Test collapsed
st.all <- collapseNoMismatch(st.all, minOverlap = 20, orderBy = "abundance",
                                  vec = TRUE, verbose = TRUE)

#Remove chimeras
seqtab.nochim <- removeBimeraDenovo(st.all, method="consensus", multithread=TRUE, verbose=TRUE)

#Check output of chimera removal
print(paste(sum(seqtab.nochim)/sum(st.all),"of the abundance remaining after chimera removal"))

#Check complexity
hist(seqComplexity(seqtab.nochim), 100)

#Look at seqlengths
plot(table(nchar(getSequences(seqtab.nochim))))

#cut to expected size allowing for some codon indels
seqtab.nochim <- seqtab.nochim[,nchar(colnames(seqtab.nochim)) %in% 200:210]

#Filter for stop codons
seqs <- DNAStringSet(getSequences(seqtab.nochim))
codon_filt <- taxreturn::codon_filter(seqs)
seqtab.nochim <- seqtab.nochim[,colnames(seqtab.nochim) %in% codon_filt]

#Filter for homology with the target marker
fwh_ref <-  ape::read.dna("reference/fwh_insecta_aligned_curated.fasta", format="fasta")
model <- aphid::derivePHMM(fwh_ref)

seqs <- as.DNAbin(DNAStringSet(colnames(seqtab.nochim)))
homology_filt <- taxreturn::clean_seqs(seqs, minscore = 100, shave = FALSE, model = model)

seqtab.nochim <-  seqtab.nochim[,colnames(seqtab.nochim) %in% homology_filt]

dir.create("output/rds/")
saveRDS(seqtab.nochim, "output/rds/seqtab_final.rds") # CHANGE ME to where you want sequence table saved
```


## Assign taxonomy with IDTAXA & Exact matching

We will use the IDTAXA algorithm of Murali et al 2018 - https://doi.org/10.1186/s40168-018-0521-5

For alternative options such as RDP or blast assignment, see:
I would strongly recommend not using BLAST due to potential for false positives 

IDTAXA requires training on a curated reference database - The pre-trained file can be found in the reference folder, alternatively see the taxreturn scripts to curate a reference database and train a new classifier.

Folllowing assignment with IDTAXA, we will also use exact matching with a reference database to assign more sequences (including the synthetic positive controls) to species level

```{r IDTAXA}
#Run
seqtab.nochim <- readRDS("output/rds/seqtab_final.rds")

trainingSet <- readRDS("reference/merged_arthropoda_idtaxa.rds")
dna <- DNAStringSet(getSequences(seqtab.nochim)) # Create a DNAStringSet from the ASVs

##Decide on threshold
ids <- IdTaxa(dna, trainingSet, processors=10,threshold = 60, verbose=TRUE)  #WARNING - assigning more than one processor currently crashes R

writeRDS(ids, "ids.RDS")
#plot(ids, trainingSet)


#delete existing file
cat("",file="idtaxa.csv")
for (i in 1:length(ids)){
 lines <- as.data.frame(t(cbind(ids[[i]]$taxon,ids[[i]]$confidence)))
 rownames(lines) <- c("taxa","confidence")
write.table(lines,file="idtaxa.csv",sep=",",append=TRUE, col.names=FALSE)
}

ranks <-  c("Kingdom", "Phylum","Class", "Order", "Family", "Genus","Species") # ranks of interest
#Convert the output object of class "Taxa" to a matrix analogous to the output from assignTaxonomy
tax <- t(sapply(ids, function(x) {
        taxa <- x$taxon
        taxa[startsWith(taxa, "unclassified_")] <- NA
        taxa
}))

library(stringi)
tax <- stri_list2matrix(lapply(tax, unlist), byrow=TRUE, fill=NA)

#Add sequences and column names to matrix
colnames(tax) <- ranks; rownames(tax) <- getSequences(seqtab.nochim)

#Subset to remove the root rank
tax <- subset(tax, select=c("Kingdom", "Phylum","Class", "Order", "Family", "Genus","Species"))

#Propagate high order ranks to unassigned ASV's
tax <- propagate_tax(tax,from="Phylum") 


#Check Output
taxa.print <- tax # Removing sequence rownames for display only
rownames(taxa.print) <- NULL
head(taxa.print)

# Write taxonomy table to disk
saveRDS(tax, "output/rds/tax_IdTaxa.rds") 

tax <- readRDS("output/rds/tax_IdTaxa.rds") 

#Add missed species using exact matching

#
exact <- assignSpecies(seqtab.nochim, "reference/merged_rdp_species_synsadded.fa.gz", allowMultiple = TRUE, tryRC = TRUE, verbose = FALSE)

exact <- exact %>% 
  as_tibble() %>%
  mutate(binomial =  case_when(!is.na(Species) ~  paste0(Genus,"_",Species)))

###Assign synthetics using exact matching

exact <- assignSpecies(seqtab.nochim, "reference/inhouse_syns.fa", allowMultiple = TRUE, tryRC = TRUE, verbose = FALSE)

exact <- exact %>% 
  as_tibble() %>%
  mutate(binomial =  case_when(!is.na(Species) ~  paste0(Genus,"_",Species)))


#merge together
#For exact where Species is not NA, replace tax$Species where Species contains K__,P__,C__,O__,F__,G__
pattern <- c("K__","P__","C__","O__","F__","G__")
for (row in 1:nrow(tax)){
  if   (str_detect(tax[row,7], paste(pattern, collapse="|")) && !is.na(exact$binomial[row]) == TRUE ) {
  tax[row,7] <- exact$binomial[row]
  }
}

# Write taxonomy table to disk
saveRDS(tax, "output/rds/tax_IdTaxaExact.rds") 

```

## Make phylogenetic tree

```{r phylogenetic tree}
#seqtab.nochim <- readRDS("output/rds/seqtab_final_Run2.rds")

seqs <- getSequences(seqtab.nochim)
names(seqs) <- seqs # This propagates to the tip labels of the tree
alignment <- AlignSeqs(DNAStringSet(seqs), anchor=NA)

library(phangorn)
phang.align <- phyDat(as(alignment, "matrix"), type="DNA")
dm <- dist.ml(phang.align)
treeNJ <- NJ(dm) # Note, tip order != sequence order
fit = pml(treeNJ, data=phang.align)

## negative edges length changed to 0!

fitGTR <- update(fit, k=4, inv=0.2)
fitGTR <- optim.pml(fitGTR, model="GTR", optInv=TRUE, optGamma=TRUE,
                      rearrangement = "stochastic", control = pml.control(trace = 0))
detach("package:phangorn", unload=TRUE)

# Write taxonomy table to disk
saveRDS(fitGTR, "phytree.rds") 

```


## Track reads through process

```{r }

bbdemux_reads <- read_tsv("logs/bbdemux_tidy.tsv") %>% 
  separate(sample, into="sample", sep="_S", extra= "drop") %>%
  separate(sample, into=c("dir", "sample"), sep="X_", extra= "drop") %>%
  select(-dir) %>%
  dplyr::rename(input_reads_demux = input_reads)%>%
  dplyr::rename(input_bases_demux = input_bases)


bbtrim_reads <- read_tsv("logs/bbtrim_tidy.tsv") %>%
  separate(sample, into=c("sample", "rep"), sep="_Rep", extra= "drop")%>% 
  separate(sample, into="sample", sep="_S", extra= "drop") %>%
  separate(sample, into=c("dir", "sample"), sep="X_", extra= "drop")%>%
  select(-dir)  %>%
  dplyr::rename(input_reads_trim = input_reads)%>%
  dplyr::rename(input_bases_trim = input_bases)
  
sample_tracker <- right_join(bbdemux_reads, bbtrim_reads)

#could make the above functions parse out objects in a list (ie one object for each quality) and then i join them to the samplesheet here
 
```


## Make Phyloseq object

Following taxonomic assignment, the sequence table and taxonomic table are merged into a single phyloseq object alongside the sample info csv.

We then make a plot to evaluate the effectiveness of taxonomic assignment to each rank

```{r create PS, eval = FALSE}
seqtab.nochim <- readRDS("output/rds/seqtab_final.rds")

#Fix seqtab names -removing read name, sample number etc
rownames(seqtab.nochim) <- rownames(seqtab.nochim) %>% 
  str_split_fixed("_",n=Inf) %>%
    as_tibble() %>% 
  separate(V7, into="rep", sep = "\\.", extra = "drop") %>%
  unite(col=SampleID, c("V2","rep"),sep="-") %>%
  pull(SampleID) %>%
  str_replace(pattern="Rep", replacement="rep")

tax_plus <- readRDS("output/rds/tax_IdTaxaExact.rds") 

#Load sample information
## ---- samdat ----
samdf <- read.csv("sample_data/Sample_info.csv", header=TRUE) %>%
  filter(!duplicated(sample_id)) %>%
  filter(FCID== "HLVKYDMXX") %>% # change for other runs
  magrittr::set_rownames(.$sample_id) %>%
  dplyr::select(c("sample_id", "ExtractID",
                  "geo_loc_name", "material", 
                  "target_subfragment", "F_primer", "R_primer",
                  "FCID", "seq_platform_ID"))
#Display samDF
head(samdf)

## ---- phyloseq ----
ps <- phyloseq(tax_table(tax_plus), sample_data(samdf),
               otu_table(seqtab.nochim, taxa_are_rows = FALSE))

if(nrow(seqtab.nochim) > nrow(sample_data(ps))){warning("Warning: All samples not included in phyloseq object, check sample names match the sample metadata")}

rownames(samdf)[which(!rownames(sample_data(ps))  %in% rownames(samdf))]

saveRDS(ps, "output/rds/ps_idtaxaExact.rds") 

#Rename synthetic orders
tax_table(ps)[,2][which(str_detect(tax_table(ps)[,7], "Synthetic"))] <- "Arthropoda"

ps <- ps %>%
  subset_samples(material %in% c("Drosophila Adults", "Drosophila Larvae", "Mixed Adults", "Mixed Larvae", "Synthetic", "Blank")) %>%
  subset_taxa(Phylum == "Arthropoda") %>%
  filter_taxa( function(x) mean(x) > 0, TRUE) 

dir.create("output/csv")
dir.create("output/csv/unfiltered/")

##Export raw csv
export <- speedyseq::psmelt(ps) %>%
  filter(Abundance > 0)
write.csv(export, file = "output/csv/rawdata.csv")

#Summary export
library(data.table)
summarise_taxa(ps, "Species", "sample_id") %>%
  filter(str_detect(sample_id, "NTC")) %>%
  #filter(str_detect(Species, "Drosophila|Scaptodrosophila")) %>%
  spread(key="sample_id", value="totalRA") %>%
  write.csv(file = "output/csv/unfiltered/spp_sum.csv")

summarise_taxa(ps, "Genus", "sample_id") %>%
  spread(key="sample_id", value="totalRA") %>%
  write.csv(file = "output/csv/unfiltered/gen_sum.csv")

##Output fasta of all ASV's - Name each one by abundance + Taxonomic assignment

ps_to_fasta(ps, "output/all_taxa.fasta", rank="Species")
```


### Summarise taxonomic assignment

```{r sum taxa}
#Fraction of reads assigned to each taxonomic rank
sum_reads <- speedyseq::psmelt(ps) %>%
  gather("Rank","Name", rank_names(ps)) %>%
  group_by(Rank) %>% 
  mutate(Name = replace(Name, str_detect(Name, "__"),NA)) %>% # This line turns the "__" we added to lower ranks back to NA's
  summarize(Reads_classified = sum(Abundance * !is.na(Name))) %>%
  mutate(Frac_reads = Reads_classified / sum(sample_sums(ps))) %>%
  mutate(Rank = factor(Rank, rank_names(ps))) %>%
  arrange(Rank)

#Fraction of ASV's assigned to each taxonomic rank
sum_otu <- tax_table(ps) %>%
  as("matrix") %>%
  as_tibble(rownames="OTU") %>%
  gather("Rank","Name",rank_names(ps)) %>%
  group_by(Rank) %>%
  mutate(Name = replace(Name, str_detect(Name, "__"),NA)) %>% # This line turns the "__" we added to lower ranks back to NA's
  summarize(OTUs_classified = sum(!is.na(Name))) %>%
  mutate(Frac_OTUs = OTUs_classified / ntaxa(ps)) %>%
  mutate(Rank = factor(Rank, rank_names(ps))) %>%
  arrange(Rank)

print(sum_reads)
print(sum_otu)


ps_test <- speedyseq::tax_glom(ps, "Species")
sum_test <- tax_table(ps_test) %>%
  as("matrix") %>%
  as_tibble(rownames="OTU") %>%
  gather("Rank","Name",rank_names(ps)) %>%
  group_by(Rank) %>%
  mutate(Name = replace(Name, str_detect(Name, "__"),NA)) %>% # This line turns the "__" we added to lower ranks back to NA's
  summarize(OTUs_classified = sum(!is.na(Name))) %>%
  mutate(Frac_OTUs = OTUs_classified / ntaxa(ps)) %>%
  mutate(Rank = factor(Rank, rank_names(ps))) %>%
  arrange(Rank)


unique_sp <- unique(tax_table(ps_test)[,7]) %>% unname() %>% as.data.frame() %>% filter(!str_detect(V1, "__"))

```
