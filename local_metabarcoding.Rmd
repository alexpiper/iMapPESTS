---
title: "iMapPESTS local metabarcoding workflow V2"
author: "A.M. Piper"
date: "2020/04/12"
output:
  
  html_document:
    highlighter: null
    theme: "flatly"
    code_download: true
    toc: true
    toc_float: 
      collapsed: false
      smooth_scroll: true
    df_print: paged    
  pdf_document: default
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
# Knitr global setup - change eval to true to run code
library(knitr)
knitr::opts_chunk$set(echo = TRUE, eval=FALSE, message=FALSE,error=FALSE, fig.show = "hold", fig.keep = "all")
opts_chunk$set(dev = 'png')
```

# Introduction 


# Demultiplex sequencing reads
For this workflow to run, we need to first demultiplex the miseq run again as the miseq does not put indexes in fasta headers by defualt

BASH:
```{bash demultiplex 1 mismatch}
#load module
module load bcl2fastq2/2.20.0-foss-2018b

#raise amount of available file handles
ulimit -n 4000

#BCL2fastq
bcl2fastq -p 12 --runfolder-dir /group/sequencing/200203_M03633_0388_000000000-CJL7D \
--output-dir /group/pathogens/Alexp/Metabarcoding/imappests/CJL7D \
--sample-sheet /group/pathogens/Alexp/Metabarcoding/imappests/CJL7D/SampleSheet.csv \
--no-lane-splitting --barcode-mismatches 1

```


After the run has been demultiplexed, copy the following files into a new folder within the data directory, and name this folder for by the flowcell used:
* Demultiplexed forward and reverse reads
* Undetermined forward and reverse reads

Also copy these files from the original run directory in group/sequencing to this directory
* InterOp directory
* runInfo.xml
* runParameters.xml

The directory structure should be as follows

    root/
    ├── data/
    │   ├── CJL7D/
    │   │  ├── R1.fastq.gz
    │   │  ├── R2.fastq.gz
    │   │  ├── runInfo.xml
    │   │  ├── runParameters.xml
    │   │  ├── SampleSheet.csv
    │   │  └── InterOp/
    │   └── run2/
    ├── sample_data/
    ├── output/
    └── doc/


# R install and setup directories

This pipeline requires various R packages to be installed prior to running. These are obtained from CRAN, Bioconductor and Github. The seqateurs R package also provides wrappers around other software packages for QC. For convenience we will download and install these software in a new folder called "bin"

```{r install & Load packages} 
#Set required packages
.cran_packages <- c("ggplot2",
                    "gridExtra",
                    "tidyverse", 
                    "stringdist",
                    "patchwork",
                    "vegan",
                    "seqinr",
                    "patchwork",
                    "stringi",
                    "phangorn")
.bioc_packages <- c("dada2",
                    "phyloseq",
                    "DECIPHER",
                    "Biostrings",
                    "ShortRead",
                    "savR",
                    "ngsReports")

.inst <- .cran_packages %in% installed.packages()
if(any(!.inst)) {
   install.packages(.cran_packages[!.inst])
}
.inst <- .bioc_packages %in% installed.packages()
if(any(!.inst)) {
  if (!requireNamespace("BiocManager", quietly = TRUE))
    install.packages("BiocManager")
  BiocManager::install(.bioc_packages[!.inst], ask = F)
}

#Load all packages
sapply(c(.cran_packages,.bioc_packages), require, character.only = TRUE)

devtools::install_github("alexpiper/seqateurs")
library(seqateurs)

#Install bbmap
bbmap_install(dest.dir = "bin")

#Install fastqc
fastqc_install(dest.dir = "bin")

# Create directories
if(!dir.exists("data")){dir.create("data", recursive = TRUE)}
if(!dir.exists("reference")){dir.create("reference", recursive = TRUE)}
if(!dir.exists("output/logs")){dir.create("output/logs", recursive = TRUE)}
if(!dir.exists("output/csv")){dir.create("output/csv", recursive = TRUE)}
if(!dir.exists("output/rds")){dir.create("output/rds", recursive = TRUE)}
```

## Set analysis parameters and create sample sheet

In order to track samples and relevant QC statistics throughout the metabarcoding pipeline, we will first create a new samplesheet from our input samplesheets. This function requires both the SampleSheet.csv used for the sequencing run, and the runParameters.xml, both of which can be obtained from the demultiplexed sequencing run folder/

We will also add the relevant primers at this stage to this sheet:

    FORWARD PRIMERS:
        Name                    Illumina overhang adapter           Primer sequences
        fwhF2_P5	ACACTCTTTCCCTACACGACGCTCTTCCGATCT	  GGDACWGGWTGAACWGTWTAYCCHCC
    
    REVERSE PRIMER:
        Name                    Illumina overhang adapter           Primer sequences
        fwhR2n_P7	GTGACTGGAGTTCAGACGTGTGCTCTTCCGATCT	GTRATWGCHCCDGCTARWACWGG

```{r create samplesheet}
runs <- dir("data/") #Find all directories within data
SampleSheets <- paste0("data/", runs, "/SampleSheet.csv")
runParameters <- paste0("data/", runs, "/runParameters.xml")

## Need to allow create samplesheet to read in multiple!
samdf <- seqateurs::create_samplesheet(SampleSheet = SampleSheets, runParameters = runParameters)

# Add primers to samdf - 2 different reverse primers were used
samdf <- samdf %>% 
  mutate(Fprimer = "GGDACWGGWTGAACWGTWTAYCCHCC",
         Rprimer = case_when(
           str_detect(Sample_ID, "P1") ~ "GTRATWGCHCCDGCTARWACWGG",
           str_detect(Sample_ID, "P2") ~ "TATDGTRATDGCHCCNGC"
           ))

write_csv(samdf, "sample_data/Sample_info.csv")
```

# Quality checks:

We will conduct 3 quality checks. Firstly a check of the entire sequence run, followed by a sample level quality check to identify potential issues with specific samples. And then a calculation of the index switching rate by summarising correctly assigned vs missasigned indices.

```{r QC}
#Load sample sheet
samdf <- read.csv("sample_data/Sample_info.csv", stringsAsFactors = FALSE)
runs <- unique(samdf$FCID)

# Set parameters
barcodemismatch <- 1 # set the barcode mismatch used during demultiplexing (illumina default is 1)

for (i in 1:length(runs)){
  ## Run level quality check using savR
  path <- paste0("data/", runs[i], "/")
  fc <- savR(path)
  qc.dir <- paste0("output/logs/", runs[i],"/" )
  dir.create(qc.dir, recursive = TRUE)
  write_csv(correctedIntensities(fc), paste0(qc.dir, "correctedIntensities.csv"))
  write_csv(errorMetrics(fc), paste0(qc.dir, "errorMetrics.csv"))
  write_csv(extractionMetrics(fc), paste0(qc.dir, "extractionMetrics.csv"))
  write_csv(qualityMetrics(fc), paste0(qc.dir, "qualityMetrics.csv"))
  write_csv(tileMetrics(fc), paste0(qc.dir, "tileMetrics.csv"))

  avg_intensity <- fc@parsedData[["savCorrectedIntensityFormat"]]@data %>%
    group_by(tile, lane) %>%
    summarise(Average_intensity = mean(avg_intensity)) %>% 
    ungroup() %>%
    mutate(side = case_when(
      str_detect(tile, "^11") ~ "Top",
      str_detect(tile, "^21") ~ "Bottom"
        ))%>%
    ggplot(aes(x=lane, y=as.factor(tile), fill=Average_intensity)) +
    geom_tile() +
    facet_wrap(~side, scales="free") +
    scale_fill_viridis_c()
  
  pdf(file=paste(qc.dir, "/avgintensity.pdf", sep=""), width = 11, height = 8 , paper="a4r")
  plot(avg_intensity)
  try(dev.off(), silent=TRUE)
  
  pdf(file=paste(qc.dir, "/PFclusters.pdf", sep=""), width = 11, height = 8 , paper="a4r")
  pfBoxplot(fc)
  try(dev.off(), silent=TRUE)

  for (lane in 1:fc@layout@lanecount) {
  pdf(file=paste(qc.dir, "/QScore_L", lane, ".pdf", sep=""), width = 11, height = 8 , paper="a4r")
      qualityHeatmap(fc, lane, 1:fc@directions)
  try(dev.off(), silent=TRUE)
  } 
}

## Sample level quality check using fastqc
for (i in 1:length(runs)){
  path <- paste0("data/", runs[i], "/")
  qc.dir <- paste0("output/logs/", runs[i],"/FASTQC" )
  dir.create(qc.dir, recursive=TRUE)
  qc_out <- seqateurs::fastqc(fq.dir = path, qc.dir	= qc.dir, fastqc.path = "bin/FastQC/fastqc", threads=2)
  writeHtmlReport(qc.dir, overwrite = TRUE, gcType ="Genome",  quiet=FALSE)
}

## Calculate index switching
for (i in 1:length(runs)){
  path <- paste0("data/", runs[i], "/")
  qc.dir <- paste0("output/logs/", runs[i] )
  run_data <- samdf %>%
    filter(FCID == runs[i])

  indices <- sort(list.files(path, pattern="_R1_", full.names = TRUE)) %>%
    purrr::set_names() %>%
    purrr::map(seqateurs::summarise_index) %>%
    bind_rows(.id="Sample_Name")%>%
    arrange(desc(Freq)) %>% 
    dplyr::mutate(Sample_Name = Sample_Name %>% 
                    str_replace(pattern = "^(.*)\\/", replacement="") %>%
                    str_replace(pattern = "(?:.(?!_R1_))+$", replacement=""))

  combos <- indices %>% 
    filter(!str_detect(Sample_Name, "Undetermined")) %>%
    select(index, index2) %>%
    expand(index, index2)

  #get unused combinations resulting from index switching
  switched <- left_join(combos, indices, by=c("index", "index2")) %>%
    drop_na()
  
  other_reads <- anti_join(indices,combos, by=c("index", "index2")) %>%
    summarise(sum = sum(Freq)) %>%
    pull(sum)
  
  ##Summary of index switching rate
  exp_rate <- switched %>% 
    filter(!str_detect(Sample_Name, "Undetermined"))
  obs_rate <- switched %>% 
    filter(str_detect(Sample_Name,"Undetermined"))
  switch_rate <- (sum(obs_rate$Freq)/sum(exp_rate$Freq))
    message("Index switching rate calculated as: ", switch_rate)

  #Plot switching
    gg.switch <- ggplot(data = switched, aes(x = index, y = index2), stat="identity") +
    geom_tile(aes(fill = Freq),alpha=0.8)  + 
    scale_fill_viridis_c(name="log10 Reads", begin=0.1, trans="log10")+
    theme(axis.text.x = element_text(angle=90, hjust=1), 
          plot.title=element_text(hjust = 0.5),
          plot.subtitle =element_text(hjust = 0.5)#,
          #legend.position = "none"
          ) +
    scale_x_discrete(limits=index)+
    scale_y_discrete(limits=index2)+
    labs(title= runs[i], subtitle = paste0(
      "Total Reads: ", sum(indices$Freq),
      ", Switch rate: ", sprintf("%1.4f%%", switch_rate*100),
      ", other Reads: ", other_reads)) 
  pdf(file=paste(qc.dir, "/switchrate.pdf", sep=""), width = 11, height = 8 , paper="a4r")
      plot(gg.switch)
  try(dev.off(), silent=TRUE)
  
  }
```

# Trim Primers

DADA2 requires Non-biological nucleotides i.e. primers, adapters, linkers, etc to be removed. Following demultiplhowever primer sequences still remain in the reads and must be removed prior to use with the DADA2 algorithm. 
For this workflow we will be using the Kmer based adapter trimming software BBDuk (Part of BBTools package https://jgi.doe.gov/data-and-tools/bbtools/) to trim the primers from our raw data files. the seqateurs R package contains a wrapper to call bbduk from R to trim primers.

```{r primer trimming , message=FALSE}
#Load sample sheet
samdf <- read.csv("sample_data/Sample_info.csv", stringsAsFactors = FALSE)
runs <- unique(samdf$FCID)

#Create lists to track reads
trimmed <- vector("list", length = length(runs))

i=1
for (i in 1:length(runs)){
  path <- paste0("data/", runs[i])
  qc.dir <- paste0("output/logs/", runs[i],"/" )

  run_data <- samdf %>%
    filter(FCID == runs[i])
  
  #Get primer sequences
  primers <- c(unique(run_data$Fprimer), unique(run_data$Rprimer))

  fastqFs <- sort(list.files(paste0(path), pattern="_R1_", full.names = TRUE))
  fastqRs <- sort(list.files(paste0(path), pattern="_R2_", full.names = TRUE))
  if(length(fastqFs) != length(fastqRs)) stop(paste0("Forward and reverse files for ",runs[i]," do not match."))

  trimmed[[i]] <- bbtrim(install="bin/bbmap", fwd = fastqFs, rev = fastqRs,
                primers = primers, checkpairs=TRUE,
                degenerate = TRUE, out.dir="01_trimmed", trim.end = "left",
                kmer=NULL, tpe=TRUE, tbo=TRUE,
                ordered = TRUE, mink = FALSE, hdist = 2,
                maxlength =(max(run_data$Fread, run_data$Rread) - sort(nchar(primers), decreasing = FALSE)[1]) +5,
                overwrite = TRUE, quality = FALSE, quiet=FALSE)

  # Check sequence lengths
  pre_trim <- plot_lengths(dir=path, aggregate=TRUE, sample=1e5)
  post_trim <- plot_lengths(dir=paste0(path, "/01_trimmed/"), aggregate=TRUE, sample=1e5)

  pdf(file=paste(qc.dir, "/readlengths.pdf", sep=""), width = 11, height = 8 , paper="a4r")
  plot(pre_trim)
  plot(post_trim)
  try(dev.off(), silent=TRUE)
  
  trim_summary <- trimmed[[i]] %>% 
    mutate(perc_reads_remaining = signif(((output_reads / input_reads) * 100), 2),
           perc_bases_remaining = signif(((output_bases / input_bases) * 100), 2)
           ) %>%
    filter(!is.na(perc_reads_remaining))
    
  message(paste0(signif(mean(trim_summary$perc_reads_remaining, na.rm = TRUE), 2),
                 "% of reads and ",
                 signif(mean(trim_summary$perc_bases_remaining, na.rm = TRUE), 2),
                 "% of bases remaining for ", runs[i]," after trimming"))
  
  # Print warning for each sample
  for(w in 1:nrow(trim_summary)){
    if (trim_summary[w,]$perc_reads_remaining < 10) {message(paste0("WARNING: Less than 10% bases remaining for ",trim_summary[w,]$sample), "Check primers are correct")}
  }
}

# Track reads
read_tracker <- samdf %>% 
  select(Sample_ID, FCID) %>%
  left_join(
    trimmed %>%
    purrr::set_names(runs) %>%
    bind_rows(.id="FCID") %>%
    mutate(sample = str_replace(basename(sample), pattern="_S.*$", replacement="")) %>%
    dplyr::rename(Sample_ID = sample),
  by=c("Sample_ID", "FCID"))

write_csv(read_tracker, "output/logs/read_tracker.csv")
```

# Plot read quality & lengths

```{r QA plot, eval = FALSE, cache= TRUE}
#Load sample sheet
samdf <- read.csv("sample_data/Sample_info.csv", stringsAsFactors = FALSE)
runs <- unique(samdf$FCID)

# Plotting parameters
readQC_aggregate <- TRUE
readQC_subsample <-  12

amplicon = 205 # Set to maximum size between the two primers. If working with variable barcode lengths, set to readlength

for (i in 1:length(runs)){
  run_data <- samdf %>%
    filter(FCID == runs[i])

  path <- paste0("data/", runs[i], "/01_trimmed" )
 
  ##Get trimmed files, accounting for empty files (28 indicates empty sample)
  trimmedFs <- sort(list.files(path, pattern="_R1_", full.names = TRUE))
  trimmedFs <- trimmedFs[file.size(trimmedFs) > 28]

  #Choose a random subsample for quality checks
  sampleF <- sample(trimmedFs, readQC_subsample) #NOTE - need to have option to pass
  sampleR <- sampleF %>% str_replace(pattern="_R1_", replacement = "_R2_")
  
  #Estimate an optimat trunclen
  truncLen <- estimate_trunclen(sampleF, sampleR, maxlength=amplicon)

  #Plot qualities
  gg.Fqual <- plot_quality(sampleF) +
    geom_vline(aes(xintercept=truncLen[1]), colour="blue") +
    annotate("text", x = truncLen[1]-10, y =2, label = paste0("Suggested truncLen = ", truncLen[1]), colour="blue") +
    ggtitle(paste0(runs[i], " Forward Reads")) +
    scale_x_continuous(breaks=seq(0,300,25))
  gg.Fee <- plot_maxEE(sampleF) + 
    geom_vline(aes(xintercept=truncLen[1]), colour="blue")+
    annotate("text", x = truncLen[1]-10, y =-3, label = paste0("Suggested truncLen = ", truncLen[1]), colour="blue") +
    ggtitle(paste0(runs[i], " Forward Reads")) +
    scale_x_continuous(breaks=seq(0,300,25)) +
    theme(legend.position = "bottom")
  gg.Rqual <- plot_quality(sampleR) + 
    geom_vline(aes(xintercept=truncLen[2]), colour="blue")+
    annotate("text", x = truncLen[1]-10, y =2, label = paste0("Suggested truncLen = ", truncLen[2]), colour="blue") +
    ggtitle(paste0(runs[i], " Reverse Reads")) +
    scale_x_continuous(breaks=seq(0,300,25)) +
    theme(legend.position = "bottom")
  gg.Ree <- plot_maxEE(sampleR) +
    geom_vline(aes(xintercept=truncLen[2]), colour="blue")+
    annotate("text", x = truncLen[1]-10, y =-3, label = paste0("Suggested truncLen = ", truncLen[2]), colour="blue") +
    ggtitle(paste0(runs[i], " Reverse Reads")) +
    scale_x_continuous(breaks=seq(0,300,25)) +
    theme(legend.position = "bottom")

  Qualplots <- (gg.Fqual + gg.Rqual) / (gg.Fee + gg.Ree)
  
  #output plots
  pdf(paste0("output/logs/",runs[i],"/",runs[i], "_prefilt_quality.pdf"), width = 11, height = 8 , paper="a4r")
  plot(Qualplots)
  try(dev.off(), silent=TRUE)
}
```

This has output a prefilt_quality.pdf plot for each of the runs analysed in the logs folder. On the top is the quality score per cycle, and on the bottom is the cumulative expected errors (calculated as EE = sum(10^(-Q/10)) on a log scale. For the quality plot, the median quality score at each position is shown by the green line, and the quartiles of the quality score distribution by the orange lines. For the maxEE lines, the red lines showing the expected error filter options. The blue vertical line on both plots shows the suggested truncLen option automatically determined. 

Ensure that the blue suggested trunclen looks reasonable before continuing. Truncating length will reduce the number of reads violating the expected error filter, and therefore increase the number of reads proceding through the pipeline. The reverse reads will generally have lower quality, and therefore a lower truncLen than the forward reads.

# Filter reads

This stage will use read truncation and max expected error function to remove low quality reads and read tails. All reads containing N bases will also be removed. this will output _postfilt_quality.pdf in the logs folder to determine how sucessfull it has been in cleaning up the quality.

```{r filter and trim}
samdf <- read.csv("sample_data/Sample_info.csv", stringsAsFactors = FALSE)
runs <- unique(samdf$FCID)
filtered_out <- vector("list", length=length(runs))

# Set important variables for trimming
maxEE <- 1 #Filter reads above Expected errors (EE = sum(10^(-Q/10))). Set higher for poor quality sequences.
rm.lowcomplex <- 0 # Remove low-complexity, set higher for NovaSeq and other 2 colour platforms
amplicon = 205 # Set to maximum size between the two primers. If working with variable barcode lengths, set to readlength

# Estimate best length to truncate forward and reverse reads to
#truncLen <- estimate_trunclen(sampleF, sampleR, maxlength=amplicon)

for (i in 1:length(runs)){
  
   run_data <- samdf %>%
    filter(FCID == runs[i])
  
  path <- paste0("data/", runs[i], "/01_trimmed" )
  
  filtpath <- paste0("data/", runs[i], "/02_filtered" ) # Filtered forward files go into the path/filtered/ subdirectory
  dir.create(filtpath)
  
  fastqFs <- sort(list.files(path, pattern="R1_001.*"))
  fastqRs <- sort(list.files(path, pattern="R2_001.*"))
  
  if(length(fastqFs) != length(fastqRs)) stop(paste0("Forward and reverse files for ",runs[i]," do not match."))
  
  filtered_out[[i]] <- filterAndTrim(fwd = file.path(path, fastqFs), filt = file.path(filtpath, fastqFs),
                                      rev = file.path(path, fastqRs), filt.rev = file.path(filtpath, fastqRs),
                                      maxEE = maxEE, truncLen = truncLen, rm.lowcomplex = rm.lowcomplex,
                                      rm.phix = TRUE, matchIDs = TRUE, id.sep = "\\s",
                                      multithread = TRUE, compress = TRUE, verbose = TRUE)

  # post filtering plot
  filtFs <- sort(list.files(filtpath, pattern="R1_001.*", full.names = TRUE))
  sampleF <- sample(filtFs, readQC_subsample)
  sampleR <- sampleF %>% str_replace(pattern="R1_001", replacement = "R2_001")
  
  p1 <- plotQualityProfile(sampleF, aggregate = readQC_aggregate) +
    ggtitle(paste0(runs[i]," Forward Reads")) +
    scale_x_continuous(breaks=seq(0,300,25))
  p2 <- plotQualityProfile(sampleR, aggregate = readQC_aggregate) + 
    ggtitle(paste0(runs[i]," Reverse Reads"))+
    scale_x_continuous(breaks=seq(0,300,25))
  
  #output plots
  if (!dir.exists("output/logs/")){ dir.create("output/logs/")}
  pdf(paste0("output/logs/", runs[i],"/",runs[i], "_postfilt_quality.pdf"), width = 11, height = 8 , paper="a4r")
  plot(p1)
  plot(p2)
  try(dev.off(), silent=TRUE)
  
  filtered_summary <- filtered_out[[i]] %>% 
    as.data.frame() %>%
    rownames_to_column("sample") %>%
    mutate(reads_remaining = signif(((reads.out / reads.in) * 100), 2)) %>%
    filter(!is.na(reads_remaining))
    
  message(paste0(signif(mean(filtered_summary$reads_remaining, na.rm = TRUE), 2), "% of reads remaining for ", runs[i]," after filtering"))
  
  # Print warning for each sample
  for(w in 1:nrow(filtered_summary)){
    if (filtered_summary[w,]$reads_remaining < 10) {
      message(paste0("WARNING: Less than 10% reads remaining for ", trim_summary[w,]$sample), "Check filtering parameters ")
    } 
  }
  
}

# Track reads
read_tracker <- read_csv("output/logs/read_tracker.csv") %>% 
  left_join(
    filtered_out %>%
    map(as_tibble, rownames=NA) %>%
    map(rownames_to_column, var="Sample_ID") %>%
    purrr::set_names(runs) %>%
    bind_rows(.id="FCID") %>%
    mutate(Sample_ID = str_replace(basename(Sample_ID), pattern="_S.*$", replacement="")) %>%
    dplyr::rename(filter_input = reads.in, filter_output = reads.out),
  by=c("Sample_ID", "FCID"))

write_csv(read_tracker, "output/logs/read_tracker.csv")
```


# Infer sequence variants for each run

This workflow uses the DADA2 algorithm to differentiate real sequences from error using their abundance and co-occurance patters. This relies on the assumption of a random error process where base errors are introduced randomly by either PCR polymerase or sequencing, real sequences will be high quality in the same way, while bad sequences are bad in different individual ways. DADA2 depends on a parameterized error model (the 16(possible bases) × 41(phred score) transition probabilities, for example, p(A→C, 35)), which is estimated from the data. DADA2’s default parameter estimation method is to perform a weighted loess fit to the regularized log of the observed mismatch rates as a function of their quality, separately for each transition type (for example, A→C mismatches are fit separately from A→G mismatches). Following error model learning, all identical sequencing reads are dereplicated into into “Amplicon sequence variants” (ASVs) with a corresponding abundance equal to the number of reads with that unique sequence. The forward and reverse reads are then merged together by aligning the denoised forward reads with the reverse-complement of the corresponding reverse reads, and then constructing the merged “contig” sequences. Following this step, a sequence variant table is constructed and saved as an RDS file.

```{r DADA}
set.seed(100) # set random seed for reproducability
samdf <- read.csv("sample_data/Sample_info.csv", stringsAsFactors = FALSE)
runs <- unique(samdf$FCID)

# Set parameters
nbases = 1e+08 # Minimum number of total bases to use for error rate - increase if samples are deep sequenced (>1M reads per sample)
randomize = TRUE # Pick samples randomly to learn errors
pool = "pseudo" # Higher accuracy for low abundance at expense of runtime. Set to FALSE for a faster run

dada_out <- vector("list", length=length(runs))
i=1
for (i in 1:length(runs)){
  
  run_data <- samdf %>%
    filter(FCID == runs[i])
  
  #Check if run used twin tags
  filtpath <- paste0("data/", runs[i], "/02_filtered" )
  
  filtFs <- list.files(filtpath, pattern="R1_001.*", full.names = TRUE)
  filtRs <- list.files(filtpath, pattern="R2_001.*", full.names = TRUE)
  
  # Learn error rates from a subset of the samples and reads (rather than running self-consist with full dataset)
  errF <- learnErrors(filtFs, multithread = TRUE, nbases = nbases, randomize = randomize, qualityType = "FastqQuality", verbose=TRUE)
  errR <- learnErrors(filtRs, multithread = TRUE, nbases = nbases, randomize = randomize, qualityType = "FastqQuality", verbose=TRUE)
  
  #write out errors for diagnostics
  write_csv(as.data.frame(errF$trans), paste0("output/logs/", runs[i],"/",runs[i],"_errF_observed_transitions.csv"))
  write_csv(as.data.frame(errF$err_out), paste0("output/logs/", runs[i],"/",runs[i],"_errF_inferred_errors.csv"))
  write_csv(as.data.frame(errR$trans), paste0("output/logs/", runs[i],"/",runs[i],"_errR_observed_transitions.csv"))
  write_csv(as.data.frame(errR$err_out), paste0("output/logs/", runs[i],"/",runs[i],"_errR_inferred_errors.csv"))
  
  ##output error plots to see how well the algorithm modelled the errors in the different runs
  p1 <- plotErrors(errF, nominalQ = TRUE) + ggtitle(paste0(runs[i], " Forward Reads"))
  p2 <- plotErrors(errR, nominalQ = TRUE) + ggtitle(paste0(runs[i], " Reverse Reads"))
  pdf(paste0("output/logs/", runs[i],"/",runs[i],"_errormodel.pdf"), width = 11, height = 8 , paper="a4r")
  plot(p1)
  plot(p2)
  try(dev.off(), silent=TRUE)
  
  #Error inference and merger of reads
  dadaFs <- dada(filtFs, err = errF, multithread = TRUE, pool = pool, verbose = TRUE)
  dadaRs <- dada(filtRs, err = errR, multithread = TRUE, pool = pool, verbose = TRUE)
  saveRDS(dadaFs, paste0("output/logs/", runs[i],"/", runs[i], "_dadaFs.rds"))
  saveRDS(dadaRs, paste0("output/logs/", runs[i],"/", runs[i], "_dadaRs.rds"))

  # merge reads
  mergers <- mergePairs(dadaFs, filtFs, dadaRs, filtRs, verbose = TRUE, minOverlap = 12, trimOverhang = TRUE) 
  bind_rows(mergers, .id="Sample") %>%
    mutate(Sample = str_replace(Sample, pattern="_S.*$", replacement="")) %>%
    write_csv(paste0("output/logs/",runs[i],"/",runs[i], "_mergers.csv"))
  
  #Construct sequence table
  seqtab <- makeSequenceTable(mergers)
  saveRDS(seqtab, paste0("output/rds/", runs[i], "_seqtab.rds"))

  # Track reads
  getN <- function(x) sum(getUniques(x))
  dada_out[[i]] <- cbind(sapply(dadaFs, getN), sapply(dadaRs, getN), sapply(mergers, getN)) %>%
    magrittr::set_colnames(c("dadaFs", "dadaRs", "merged")) %>%
    as.data.frame() %>%
    rownames_to_column("Sample_ID") %>%
    mutate(Sample_ID = str_replace(basename(Sample_ID), pattern="_S.*$", replacement=""))
}

# Track reads
read_tracker <- read_csv("output/logs/read_tracker.csv") %>% 
  left_join(dada_out %>%
            purrr::set_names(runs) %>%
            bind_rows(.id="FCID"),
            by=c("Sample_ID", "FCID"))

write_csv(read_tracker, "output/logs/read_tracker.csv")
```

# Merge Runs, Remove Chimeras and filter

Following denoising and merging of reads, if there were multiple flowcells of data analyse the sequence tables from these will be merged together. Next the sequences are checked for chimeras, and all sequences containing stop codons are removed. The final cleaned sequence table is saved as output/rds/seqtab_final.rds

```{r merge runs and remove chimeras}
seqtabs <- list.files("output/rds/", pattern="seqtab.rds", full.names = TRUE)

# If multiple seqtabs present, merge.
if(length(seqtabs) > 1){
  st.all <- mergeSequenceTables(tables=seqtabs)
} else if(length(seqtabs) == 1) {
  st.all <- readRDS(seqtabs)
}

#Remove chimeras
seqtab.nochim <- removeBimeraDenovo(st.all, method="consensus", multithread=TRUE, verbose=TRUE)
message(paste(sum(seqtab.nochim)/sum(st.all),"of the abundance remaining after chimera removal"))

#cut to expected size allowing for some codon indels
seqtab.cut <- seqtab.nochim[,nchar(colnames(seqtab.nochim)) %in% 200:210]
message(paste0("Identified ",
               length(colnames(seqtab.nochim))  - length(colnames(seqtab.cut)),
               " incorrectly sized sequences out of ", length(colnames(seqtab.nochim)) , " input sequences."))
message(paste(sum(seqtab.cut)/sum(seqtab.nochim),"of the abundance remaining after cutting to expected size"))

#Filter sequences containing stop codons
seqs <- DNAStringSet(getSequences(seqtab.cut))
codon_filt <- codon_filter(seqs)
seqtab.final <- seqtab.cut[,colnames(seqtab.cut) %in% codon_filt]
message(paste0("Identified ",
               length(colnames(seqtab.cut))  - length(colnames(seqtab.final)),
               " sequences containing stop codon out of ", length(colnames(seqtab.cut)) , " input sequences."))
message(paste(sum(seqtab.final)/sum(seqtab.cut),"of the abundance remaining after removing seqs with stop codons "))

saveRDS(seqtab.final, "output/rds/seqtab_final.rds")

# summarise cleanup
cleanup <- st.all %>%
  as.data.frame() %>%
  pivot_longer( everything(),
    names_to = "OTU",
    values_to = "Abundance") %>%
  group_by(OTU) %>%
  summarise(Abundance = sum(Abundance)) %>%
  mutate(length  = nchar(OTU)) %>%
  mutate(type = case_when(
    !OTU %in% getSequences(seqtab.nochim) ~ "Chimera",
    !OTU %in% getSequences(seqtab.cut) ~ "Incorrect size",
    !OTU %in% getSequences(seqtab.final) ~ "Stop codons",
    TRUE ~ "Real"
  )) 
write_csv(cleanup, "output/logs/chimera_summary.csv")

#Read Tracker
read_tracker <- read_csv("output/logs/read_tracker.csv") %>% 
  left_join(as.data.frame(cbind(rowSums(st.all),
                                rowSums(seqtab.nochim),
                                rowSums(seqtab.cut),
                                rowSums(seqtab.final))) %>%
              magrittr::set_colnames(c("seqtab", "chimera_filt", "size_filt", "seqtab_final")) %>%
              rownames_to_column("Sample_ID") %>%
              mutate(Sample_ID = str_replace(basename(Sample_ID), pattern="_S.*$", replacement="")),
            by="Sample_ID")

write_csv(read_tracker, "output/logs/read_tracker.csv")

# Output length distribution plots
gg.abundance <- ggplot(cleanup, aes(x=length, y=Abundance, fill=type))+
              geom_bar(stat="identity") + 
              ggtitle("Abundance of sequences")

gg.unique <- ggplot(cleanup, aes(x=length, fill=type))+
            geom_histogram() + 
            ggtitle("Number of unique sequences")

pdf(paste0("output/logs/seqtab_length_dist.pdf"), width = 11, height = 8 , paper="a4r")
  plot(gg.abundance / gg.unique)
try(dev.off(), silent=TRUE)
```


# Assign taxonomy with IDTAXA

Now that we have a cleaned table of sequences and their abundances across samples, we need to assign taxonomy to the sequences in order to identify taxa. We will use the IDTAXA algorithm of Murali et al 2018. IDTAXA requires training on a curated reference database and a pre-trained classifier, which can be found in the reference folder, alternatively see the taxreturn r package if you wish to curate a reference database and train a new classifier.

```{r IDTAXA}
seqtab.final <- readRDS("output/rds/seqtab_final.rds")
ranks <-  c("Root", "Kingdom", "Phylum","Class", "Order", "Family", "Genus","Species") # ranks of interest

#Classify using IDTAXA
trainingSet <- readRDS("reference/idtaxa.rds")
dna <- DNAStringSet(getSequences(seqtab.final)) # Create a DNAStringSet from the ASVs
ids <- IdTaxa(dna, trainingSet, processors=1, threshold = 60, verbose=TRUE) 

# Output plot of ids
pdf(paste0("output/logs/idtaxa.pdf"), width = 11, height = 8 , paper="a4r")
  plot(ids)
try(dev.off(), silent=TRUE)

#Convert the output object of class "Taxa" to a matrix analogous to the output from assignTaxonomy
tax <- t(sapply(ids, function(x) {
    taxa <- paste0(x$taxon,"_", x$confidence)
    taxa[startsWith(taxa, "unclassified_")] <- NA
    taxa
  })) %>%
  purrr::map(unlist) %>%
  stri_list2matrix(byrow=TRUE, fill=NA) %>%
  magrittr::set_colnames(ranks) %>%
  magrittr::set_rownames(getSequences(seqtab.final)) %>%
  as.data.frame()  %T>%
  write.csv("output/logs/idtaxa_results.csv") %>%  #Write out logfile with confidence levels
  mutate_all(str_replace,pattern="(?:.(?!_))+$", replacement="") %>%
  seqateurs::propagate_tax(from="Phylum") %>% #Propagate high order ranks to unassigned ASV'
  magrittr::set_rownames(getSequences(seqtab.final)) %>%
  as.matrix()

# Write taxonomy table to disk
saveRDS(tax, "output/rds/tax_IdTaxa.rds") 
```

# Make phylogenetic tree

In addition to taxonomic assignment, we will create a phylogenetic tree from the identified sequences to allow interpetation within a phylognetic context.

```{r phylogeny}
seqtab.final <- readRDS("output/rds/seqtab_final.rds")

seqs <- getSequences(seqtab.final)
names(seqs) <- seqs # This propagates to the tip labels of the tree
alignment <- AlignSeqs(DNAStringSet(seqs), anchor=NA)

library(phangorn)
phang.align <- phyDat(as(alignment, "matrix"), type="DNA")
dm <- dist.ml(phang.align)

#Fit NJ tree
treeNJ <- NJ(dm) # Note, tip order != sequence order
fit <- pml(treeNJ, data=phang.align)

#Fit ML tree
fitGTR <- update(fit, k=4, inv=0.2)
fitGTR <- optim.pml(fitGTR, model="GTR", optInv=TRUE, optGamma=TRUE,
                      rearrangement = "stochastic", control = pml.control(trace = 0))

# Write phytree to disk
saveRDS(fitGTR, "output/rds/phytree.rds") 

#Output newick tree
write.tree(fitGTR$tree, file="output/tree.nwk")
```

# Make Phyloseq object

Finally, we will merge the sequence table, taxonomy table, phylogenetic tree, and sample data into a single phyloseq object, filter low abundance taxa, and output summary CSV files and fasta files of the identified taxa

```{r create PS, eval = FALSE}
seqtab.final <- readRDS("output/rds/seqtab_final.rds")

#Extract start of sequence names
rownames(seqtab.final) <- str_replace(rownames(seqtab.final), pattern="_S.*$", replacement="")

tax <- readRDS("output/rds/tax_IdTaxa.rds") 

phy <- readRDS("output/rds/phytree.rds")$tree

#Load sample information
## ---- samdat ----
samdf <- read.csv("sample_data/Sample_info.csv", header=TRUE) %>%
  filter(!duplicated(Sample_ID)) %>%
  magrittr::set_rownames(.$Sample_ID) 

#Display samDF
head(samdf)

## ---- phyloseq ----
ps <- phyloseq(tax_table(tax), sample_data(samdf),
               otu_table(seqtab.final, taxa_are_rows = FALSE), phy_tree(phy))

if(nrow(seqtab.final) > nrow(sample_data(ps))){
  message("Warning: the following samples were not included in phyloseq object, check sample names match the sample metadata")
  message(rownames(seqtab.final)[!rownames(seqtab.final) %in% sample_names(ps)])
}

saveRDS(ps, "output/rds/ps.rds") 

dir.create("output/csv")
dir.create("output/csv/unfiltered/")

#Export raw csv
export <- speedyseq::psmelt(ps) %>%
  filter(Abundance > 0)
write.csv(export, file = "output/csv/rawdata.csv")

#Summary export
library(data.table)
summarise_taxa(ps, "Species", "Sample_ID") %>%
  spread(key="Sample_ID", value="totalRA") %>%
  write.csv(file = "output/csv/unfiltered/spp_sum.csv")

summarise_taxa(ps, "Genus", "Sample_ID") %>%
  spread(key="Sample_ID", value="totalRA") %>%
  write.csv(file = "output/csv/unfiltered/gen_sum.csv")

#Output fasta of all ASV's
seqateurs::ps_to_fasta(ps, "output/all_taxa.fasta", rank="Species")
```

# Output fate of reads through pipeline

```{r readtracker}
#Fraction of reads assigned to each taxonomic rank
sum_reads <- speedyseq::psmelt(ps) %>%
  gather("Rank","Name", rank_names(ps)) %>%
  group_by(Rank, Sample_ID) %>% 
  mutate(Name = replace(Name, str_detect(Name, "__"),NA)) %>% # This line turns the "__" we added to lower ranks back to NA's
  summarize(Reads_classified = sum(Abundance * !is.na(Name))) %>% 
  pivot_wider(names_from = "Rank",
              values_from = "Reads_classified")

read_tracker <- read_csv("output/logs/read_tracker.csv") %>% 
  left_join(sum_reads, by="Sample_ID")
write_csv(read_tracker, "output/logs/read_tracker.csv")


gg.readtracker <- read_tracker %>%
  dplyr::rename(trimmed_reads = output_reads) %>%
  dplyr::mutate(input_reads = input_reads / 2,
                trimmed_reads = trimmed_reads / 2) %>%
  pivot_longer(3:tail(colnames(.), 1),
               names_to = "type",
               values_to = "value") %>%
  mutate(stage= case_when(
    type %in% c("input_reads","input_bases", "ktrimmed_bases", "ktrimmed_reads", "trimmed_reads", "output_bases") ~ "BBDuk",
    type %in% c("filter_input","filter_output") ~ "FilterAndTrim",
    type %in% c("dadaFs","dadaRs", "merged") ~ "DADA",
    type %in% c("seqtab","chimera_filt", "size_filt", "seqtab_final") ~ "Seqtab",
    type %in% c("Root", "Kingdom", "Phylum", "Class","Order", "Family", "Genus", "Species") ~ "taxonomy",
  )) %>%
  ggplot(aes(x=type, y=value, fill=stage)) +
  geom_bar(stat="identity") +
  scale_x_discrete(limits = c("input_reads", "trimmed_reads", "filter_input", "filter_output",
                              "dadaFs", "dadaRs", "merged","seqtab", "chimera_filt", "size_filt", "seqtab_final",
                              "Kingdom", "Phylum", "Class","Order", "Family", "Genus", "Species")) +
  facet_wrap(~Sample_ID) +
  theme(axis.text.x = element_text(angle=90, hjust = 1, vjust=0.5)) +
  scale_fill_brewer(palette="Spectral")
  
pdf(paste0("output/logs/read_tracker.pdf"), width = 11, height = 8 , paper="a4r")
  gg.readtracker
try(dev.off(), silent=TRUE)
```

# Further analysis

From here, the dataset can be further analysed in software of your choice. I suggest the use of [phyloseq](https://joey711.github.io/phyloseq/) 

